{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e967cbfd-02db-4f34-a308-6c3acba6b95a",
   "metadata": {},
   "source": [
    "# Обучение модели Named Entity Recognition\n",
    "\n",
    "Для обучения модели я решил взять нейросеть ruBERT. Оттуда я позаимствую токенизатор и использую веса для файнтьюнинга при помощи модуля ***BertForTokenClassification*** из библиотеки *transformers*. В качестве датасета я выбрал [NERUS](https://github.com/natasha/nerus). Поверх обучения модели я использовал фреймворк *pytorch-lightning*, а для отслеживания процесса обучения фреймворк *tensorboard*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80977da-706b-48ad-a369-aeedb531f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from nerus import load_nerus\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForTokenClassification, BertTokenizerFast, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "from torchmetrics import F1Score, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf244ec-dfbf-4f02-8e28-f2ee05652286",
   "metadata": {},
   "source": [
    "## Подготовка датасета\n",
    "\n",
    "Скачаем датасет, рассмотрим пример из него и проведем подготовку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755217fa-88a5-4228-9b57-da344d98a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://storage.yandexcloud.net/natasha-nerus/data/nerus_lenta.conllu.gz -P ../datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9b9898-671a-42ed-ab36-0357db397cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NERMarkup(\n",
       "    text='Вице-премьер по социальным вопросам Татьяна Голикова рассказала, в каких регионах России зафиксирована наиболее высокая смертность от рака, сообщает РИА Новости. По словам Голиковой, чаще всего онкологические заболевания становились причиной смерти в Псковской, Тверской, Тульской и Орловской областях, а также в Севастополе. Вице-премьер напомнила, что главные факторы смертности в России — рак и болезни системы кровообращения. В начале года стало известно, что смертность от онкологических заболеваний среди россиян снизилась впервые за три года. По данным Росстата, в 2017 году от рака умерли 289 тысяч человек. Это на 3,5 процента меньше, чем годом ранее.',\n",
       "    spans=[Span(\n",
       "         start=36,\n",
       "         stop=52,\n",
       "         type='PER'\n",
       "     ),\n",
       "     Span(\n",
       "         start=82,\n",
       "         stop=88,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=149,\n",
       "         stop=160,\n",
       "         type='ORG'\n",
       "     ),\n",
       "     Span(\n",
       "         start=172,\n",
       "         stop=181,\n",
       "         type='PER'\n",
       "     ),\n",
       "     Span(\n",
       "         start=251,\n",
       "         stop=260,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=262,\n",
       "         stop=270,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=272,\n",
       "         stop=280,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=283,\n",
       "         stop=301,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=313,\n",
       "         stop=324,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=383,\n",
       "         stop=389,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=560,\n",
       "         stop=568,\n",
       "         type='ORG'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = load_nerus('../datasets/nerus_lenta.conllu.gz')\n",
    "doc = next(docs)\n",
    "doc.ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3877b98-d00f-4e24-ac32-59166a9aad20",
   "metadata": {},
   "source": [
    "Один экземпляр данных представляет собой текст и разметку в формате Span(start, stop, type):\n",
    "\n",
    "- *start* - символ начала именованной сущности в исходном тексте\n",
    "- *stop* - символ конца именованной сущности в исходном тексте\n",
    "- *type* - тип именованной сущности в исходном тексте *{'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O'}*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c741d-3334-44f4-b888-ef0959388898",
   "metadata": {},
   "source": [
    "Поскольку для обучения необходимо реализовывать метод  \\_\\_get_item\\_\\_ в классе датасета, генератор, в виде которого представлен датасет не подойдет, необходимо собрать список с разметкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "340bd31c-7a3d-4cf5-9f7c-cb88608fdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = load_nerus('../datasets/nerus_lenta.conllu.gz')\n",
    "# data = []\n",
    "# for doc in docs:\n",
    "#     data += [doc.ner]\n",
    "\n",
    "# with open('../datasets/NERUSner.pickle', 'wb') as handle:\n",
    "#         pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a370bbf2-c466-4d54-9234-95762f355756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739346"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../datasets/NERUSner.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849bce40-ba8a-4030-b5e4-30567170b3bf",
   "metadata": {},
   "source": [
    "Размер датасета составляет ~740K текстов. Для обучения возьмем меньшее количество."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6bdbd4-4001-43be-be17-a4a1a30d5e7a",
   "metadata": {},
   "source": [
    "Создадим словари для перевода лейблов в айди и наоборот."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b88177bd-6c7a-4dee-a3e3-a9aeb72de24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = {'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O'}\n",
    "labels_to_ids = {k: v for v, k in enumerate(unique_tags)}\n",
    "ids_to_labels = {v: k for v, k in enumerate(unique_tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f3b06-ac10-459e-85bb-d3ca49121d6c",
   "metadata": {},
   "source": [
    "Реализуем класс pytorch датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "445323b5-b748-4d51-ba66-55807b821dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERUSdataset(Dataset):\n",
    "    def __init__(self, data: list, tokenizer: BertTokenizerFast, max_len: int = 512):\n",
    "        self.len = len(data)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def annotate_entities(self, spans, offset_mapping):\n",
    "        '''\n",
    "        Сопоставлет аннотации с токенами, полученными из BERT токенайзера и offset_mapping\n",
    "                Параметры:\n",
    "                        spans list(Span): разметка из датасета в формате Span(start, stop, type)\n",
    "                        offset_mapping list(tuple): карта сдвигов о соответствии символов в исходном тексте и токенов в токенизированном представлении текста\n",
    "                Возвращаемое значение:\n",
    "                        annotations list(string): разметка токенов в формате {'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O'}\n",
    "        '''\n",
    "        annotations = [\"O\"] * len(offset_mapping)\n",
    "        \n",
    "        for span in spans:\n",
    "            start, stop, entity_type = span.start, span.stop, span.type\n",
    "            start_token = None\n",
    "            end_token = None\n",
    "            for i, (token_start, token_end) in enumerate(offset_mapping):\n",
    "                if token_start is not None and start >= token_start and start < token_end:\n",
    "                    start_token = i\n",
    "                if token_start is not None and stop > token_start and stop <= token_end:\n",
    "                    end_token = i + 1\n",
    "                    \n",
    "            if start_token is not None and end_token is not None:\n",
    "                annotations[start_token] = \"B-\" + entity_type\n",
    "                for i in range(start_token + 1, end_token):\n",
    "                    annotations[i] = \"I-\" + entity_type\n",
    "    \n",
    "        return annotations\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data[index].text\n",
    "        spans = self.data[index].spans\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                return_offsets_mapping=True,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                max_length=self.max_len)\n",
    "        \n",
    "        annotations = self.annotate_entities(spans, encoding[\"offset_mapping\"])\n",
    "        labels = [labels_to_ids[label] for label in annotations]\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(labels)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c4124-f236-4713-a50a-fc9e5b05022a",
   "metadata": {},
   "source": [
    "Обернем датасет в модуль *LightningDataModule*. Опишем в нем датасеты и Dataloader'ы для всех этапов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d4dc82-c167-45da-a851-b6c260c268a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERUSDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_data: list, val_data: list, test_data: list, tokenizer: BertTokenizerFast, batch_size: int = 32, max_token_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = NERUSdataset(\n",
    "            self.train_data,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        self.val_dataset = NERUSdataset(\n",
    "            self.val_data,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        self.test_dataset = NERUSdataset(\n",
    "            self.test_data,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2\n",
    "        )\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=2\n",
    "        )\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abc1b6-bc38-4aaf-b33f-96ae66571b65",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую, валидационную и тестовую выборки. Для обучения возьмем не 740К текстов, а 40К."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b53b36-fcf2-4878-aa00-8cdaec8d76a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28800, 7200, 4000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_data, test_data = train_test_split(data[:40000], test_size=0.1)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.2)\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf2b971-79bf-49bd-90d9-c881b9fb9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 512\n",
    "BERT_MODEL_NAME = 'ai-forever/ruBert-base'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL_NAME)\n",
    "data_module = NERUSDataModule(\n",
    "    train_data,\n",
    "    val_data,\n",
    "    test_data,\n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_token_len=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97671a3d-67ad-4025-a7a5-83bdf016cc72",
   "metadata": {},
   "source": [
    "## Подготовка модели\n",
    "\n",
    "Напишем модуль *LightningModule* для реализации модели. Внутри него инициализируем ***BertForTokenClassification*** с весами от *ruBERT*, указав новое количество классов в классификаторе. Реализуем все этапы обучения модели, подсчет метрик, прямой прогон через BERT и конфигурацию оптимизатора с шедулером."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44333582-c010-4fe4-93d4-f17e7f8e2fc5",
   "metadata": {},
   "source": [
    "В качестве метрики будем использовать F1-score с micro и macro усреднением. В наших данных будет наблюдатсья сильный дизбаланс классов, т.к. сущностей с меткой 'O' будет больше всего. Micro F1-score рассчитывается на уровне всего набора данных, объединяя предсказания и истинные метки для каждой сущности. Это подразумевает, что все предсказанные и истинные метки рассматриваются как один большой класс. Macro F1-score рассчитывается, усредняя F1-score для каждой индивидуальной сущности. Так мы можем равномерно оценить качество извлечения для всех сущностей, даже если некоторые из них представлены в данных редко."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5c14add-a2f6-4e15-a165-f2af74ee7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERTagger(pl.LightningModule):\n",
    "    def __init__(self, n_training_steps: int, n_warmup_steps: int, learning_rate: float):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.bert = BertForTokenClassification.from_pretrained(BERT_MODEL_NAME, num_labels=len(labels_to_ids))\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.train_f1_macro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='macro')\n",
    "        self.train_f1_micro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='micro')\n",
    "        self.val_f1_macro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='macro')\n",
    "        self.val_f1_micro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='micro')\n",
    "        self.test_f1_macro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='macro')\n",
    "        self.test_f1_micro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='micro')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = output.logits\n",
    "        loss = output.loss\n",
    "        return logits, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        logits, loss = self(input_ids, attention_mask, labels)\n",
    "\n",
    "        targets = labels.view(-1)\n",
    "        logits = logits.view(-1, len(unique_tags))\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "        \n",
    "        self.train_f1_macro(preds, targets)\n",
    "        self.train_f1_micro(preds, targets)\n",
    "        \n",
    "        self.log('train_f1_macro', self.train_f1_macro, logger=True, on_step=True, on_epoch=False)\n",
    "        self.log('train_f1_micro', self.train_f1_micro, logger=True, on_step=True, on_epoch=False)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True, sync_dist=True)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        logits, loss = self(input_ids, attention_mask, labels)\n",
    "\n",
    "        targets = labels.view(-1)\n",
    "        logits = logits.view(-1, len(unique_tags))\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "\n",
    "        self.val_f1_macro(preds, targets)\n",
    "        self.val_f1_micro(preds, targets)\n",
    "        \n",
    "        self.log('val_f1_macro', self.val_f1_micro, logger=True, on_step=True, on_epoch=False)\n",
    "        self.log('val_f1_micro', self.val_f1_micro, logger=True, on_step=True, on_epoch=False)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, sync_dist=True)\n",
    "        return loss\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        logits, loss = self(input_ids, attention_mask, labels)\n",
    "\n",
    "        targets = labels.view(-1)\n",
    "        logits = logits.view(-1, len(unique_tags))\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "\n",
    "        self.test_f1_macro(preds, targets)\n",
    "        self.test_f1_micro(preds, targets)\n",
    "        \n",
    "        self.log('test_f1_macro', self.test_f1_macro)\n",
    "        self.log('test_f1_micro', self.test_f1_micro)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d914d2fb-3276-488f-8dbf-2bc9567c7f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 9000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "steps_per_epoch=len(train_data) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66db4be-a8a8-4aab-8fa4-5e8ba03cae6d",
   "metadata": {},
   "source": [
    "Пропишем сохранение чекпоинтов и логгирование в TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad256025-fcd4-4feb-87e2-07d327adcaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"../model/checkpoints\",\n",
    "    filename=\"NERTagger-{epoch}-{step}-{val_loss:.4f}\",\n",
    "    auto_insert_metric_name=True,\n",
    "    every_n_train_steps=1000,\n",
    "    verbose=True,\n",
    "    save_top_k=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b247cff-aecf-413f-8e4c-40ade24f167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"../model/lightning_logs\", name=\"NER-tagger\")\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4a5ed-43d6-462a-ad95-4a28a0db3cdd",
   "metadata": {},
   "source": [
    "## Обучение модели\n",
    "\n",
    "Запустим обучение и протестируем обученную модель на тестовой выборке и своем предложении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c9e1787-de2a-41c1-a420-492ebdbab2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    accelerator=\"gpu\",\n",
    "    devices = [0, 1],\n",
    "    log_every_n_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e71cfd3a-088c-4cc7-a202-f9dfd21d8de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /home/jovyan/DS-Cloud-test/model/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name           | Type                       | Params\n",
      "--------------------------------------------------------------\n",
      "0 | bert           | BertForTokenClassification | 177 M \n",
      "1 | train_f1_macro | MulticlassF1Score          | 0     \n",
      "2 | train_f1_micro | MulticlassF1Score          | 0     \n",
      "3 | val_f1_macro   | MulticlassF1Score          | 0     \n",
      "4 | val_f1_micro   | MulticlassF1Score          | 0     \n",
      "5 | test_f1_macro  | MulticlassF1Score          | 0     \n",
      "6 | test_f1_micro  | MulticlassF1Score          | 0     \n",
      "--------------------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "710.888   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fcd9fb5e0c445fab6200cc47920fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "model = NERTagger(\n",
    "    learning_rate=3e-4,\n",
    "    n_warmup_steps=warmup_steps,\n",
    "    n_training_steps=total_training_steps\n",
    ")\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "374f7119-7292-4d30-a622-fc7b5e2ecd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:232: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f9c07ae8274a48ad7b21b356f73883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_f1_macro         0.9687628149986267\n",
      "      test_f1_micro         0.9949067234992981\n",
      "        test_loss          0.020064521580934525\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_f1_macro': 0.9687628149986267,\n",
       "  'test_f1_micro': 0.9949067234992981,\n",
       "  'test_loss': 0.020064521580934525}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d4188c4-0398-4782-8794-fdc9506f74d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trained_model = NERTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "trained_model.eval()\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76495fd-3aa8-4f72-898f-d917c2fdd233",
   "metadata": {},
   "source": [
    "Теперь подадим модели на вход случайное предложение и посмотрим результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69f90e69-618d-4c36-a257-d8518cd022b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beauty_visual(preds, offset_mapping):\n",
    "    prediction = ''\n",
    "    for token_pred, mapping in zip(preds, offset_mapping.squeeze().tolist()):\n",
    "        if token_pred[0][0] =='[':\n",
    "            continue\n",
    "        pred = ''\n",
    "        if token_pred[1][0] == 'B':\n",
    "            prediction += ' '\n",
    "            pred = token_pred[1][2:]\n",
    "        elif token_pred[1][0] == 'I' and token_pred[0][:2] != '##':\n",
    "            pred = '-' * len(token_pred[0]) + '-'\n",
    "        elif token_pred[0][:2] == '##' or token_pred[1][0] == 'I':\n",
    "            pred = '-' * len(token_pred[0][3:])\n",
    "        else:\n",
    "            prediction += ' '\n",
    "            pred = token_pred[1]\n",
    "        pred += (mapping[1]-mapping[0] - len(pred)) * '-'\n",
    "        prediction += pred\n",
    "    return prediction[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55ec3f1c-6ace-486f-b07f-d61ba17af1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Глава компании Apple Тим Кук осенью представил новый телефон в штаб квартире Apple Inc в Купертино\n",
      "O---- O------- ORG-- PER---- O----- O--------- O---- O------ O O--- O------- ORG------ O LOC------\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Глава компании Apple Тим Кук осенью представил новый телефон в штаб квартире Apple Inc в Купертино\"\n",
    "\n",
    "inputs = tokenizer(sentence,\n",
    "                    return_offsets_mapping=True,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors=\"pt\")\n",
    "\n",
    "ids = inputs[\"input_ids\"].to('cuda:0')\n",
    "mask = inputs[\"attention_mask\"].to('cuda:0')\n",
    "\n",
    "outputs = trained_model(ids, attention_mask=mask)\n",
    "logits = outputs[0]\n",
    "logits = logits.view(-1, len(unique_tags))\n",
    "preds = torch.argmax(logits, axis=1)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.view(-1))\n",
    "token_predictions = [ids_to_labels[i] for i in preds.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) \n",
    "\n",
    "print(sentence)\n",
    "print(beauty_visual(wp_preds, inputs[\"offset_mapping\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
