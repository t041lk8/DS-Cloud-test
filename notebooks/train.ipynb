{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e967cbfd-02db-4f34-a308-6c3acba6b95a",
   "metadata": {},
   "source": [
    "# Обучение модели Named Entity Recognition\n",
    "\n",
    "Для обучения модели я решил взять нейросеть ruBERT. Оттуда я позаимствую токенизатор и использую веса для файнтьюнинга при помощи модуля ***BertForTokenClassification*** из библиотеки *transformers*. В качестве датасета я выбрал [NERUS](https://github.com/natasha/nerus). Поверх обучения модели я использовал фреймворк *pytorch-lightning*, а для отслеживания процесса обучения фреймворк *tensorboard*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80977da-706b-48ad-a369-aeedb531f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from nerus import load_nerus\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForTokenClassification, BertTokenizerFast, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "from torchmetrics import F1Score, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf244ec-dfbf-4f02-8e28-f2ee05652286",
   "metadata": {},
   "source": [
    "## Подготовка датасета\n",
    "\n",
    "Скачаем датасет, рассмотрим пример из него и проведем подготовку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755217fa-88a5-4228-9b57-da344d98a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://storage.yandexcloud.net/natasha-nerus/data/nerus_lenta.conllu.gz -P ../datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc9b9898-671a-42ed-ab36-0357db397cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NERMarkup(\n",
       "    text='Вице-премьер по социальным вопросам Татьяна Голикова рассказала, в каких регионах России зафиксирована наиболее высокая смертность от рака, сообщает РИА Новости. По словам Голиковой, чаще всего онкологические заболевания становились причиной смерти в Псковской, Тверской, Тульской и Орловской областях, а также в Севастополе. Вице-премьер напомнила, что главные факторы смертности в России — рак и болезни системы кровообращения. В начале года стало известно, что смертность от онкологических заболеваний среди россиян снизилась впервые за три года. По данным Росстата, в 2017 году от рака умерли 289 тысяч человек. Это на 3,5 процента меньше, чем годом ранее.',\n",
       "    spans=[Span(\n",
       "         start=36,\n",
       "         stop=52,\n",
       "         type='PER'\n",
       "     ),\n",
       "     Span(\n",
       "         start=82,\n",
       "         stop=88,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=149,\n",
       "         stop=160,\n",
       "         type='ORG'\n",
       "     ),\n",
       "     Span(\n",
       "         start=172,\n",
       "         stop=181,\n",
       "         type='PER'\n",
       "     ),\n",
       "     Span(\n",
       "         start=251,\n",
       "         stop=260,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=262,\n",
       "         stop=270,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=272,\n",
       "         stop=280,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=283,\n",
       "         stop=301,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=313,\n",
       "         stop=324,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=383,\n",
       "         stop=389,\n",
       "         type='LOC'\n",
       "     ),\n",
       "     Span(\n",
       "         start=560,\n",
       "         stop=568,\n",
       "         type='ORG'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = load_nerus('../datasets/nerus_lenta.conllu.gz')\n",
    "doc = next(docs)\n",
    "doc.ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3877b98-d00f-4e24-ac32-59166a9aad20",
   "metadata": {},
   "source": [
    "Один экземпляр данных представляет собой текст и разметку в формате Span(start, stop, type):\n",
    "\n",
    "- *start* - символ начала именованной сущности в исходном тексте\n",
    "- *stop* - символ конца именованной сущности в исходном тексте\n",
    "- *type* - тип именованной сущности в исходном тексте *{'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O'}*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c741d-3334-44f4-b888-ef0959388898",
   "metadata": {},
   "source": [
    "Поскольку для обучения необходимо реализовывать метод  \\_\\_get_item\\_\\_ в классе датасета, генератор, в виде которого представлен датасет не подойдет, необходимо собрать список с разметкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "340bd31c-7a3d-4cf5-9f7c-cb88608fdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = load_nerus('../datasets/nerus_lenta.conllu.gz')\n",
    "# data = []\n",
    "# for doc in docs:\n",
    "#     data += [doc.ner]\n",
    "\n",
    "# with open('../datasets/NERUSner.pickle', 'wb') as handle:\n",
    "#         pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a370bbf2-c466-4d54-9234-95762f355756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../datasets/NERUSner.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849bce40-ba8a-4030-b5e4-30567170b3bf",
   "metadata": {},
   "source": [
    "Размер датасета составляет ~740K текстов. Для обучения возьмем меньшее количество."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6bdbd4-4001-43be-be17-a4a1a30d5e7a",
   "metadata": {},
   "source": [
    "Создадим словари для перевода лейблов в айди и наоборот."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b88177bd-6c7a-4dee-a3e3-a9aeb72de24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = {'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O'}\n",
    "labels_to_ids = {k: v for v, k in enumerate(unique_tags)}\n",
    "ids_to_labels = {v: k for v, k in enumerate(unique_tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f3b06-ac10-459e-85bb-d3ca49121d6c",
   "metadata": {},
   "source": [
    "Реализуем класс pytorch датасета. Внутри него реализована функция для перевода разметки из модуля *nerus* в разметку токенов на сущности при помощи карты сдвигов токенайзера. Это необходимо, т.к. после прогона предложения через токенайзер происходит сдвиг символов из-за добавления специальных токенов и символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "445323b5-b748-4d51-ba66-55807b821dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERUSdataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data: list, #разметка из датасета в формате NERMarkup(text, list(Spans))\n",
    "                 tokenizer: BertTokenizerFast, #токенайзер\n",
    "                 max_len: int = 512 #максимальная длина последовательности\n",
    "                ):\n",
    "        self.len = len(data)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def annotate_entities(self, spans, offset_mapping):\n",
    "        \"\"\"\n",
    "        Сопоставлет аннотации с токенами, полученными из BERT токенайзера и offset_mapping\n",
    "                Параметры:\n",
    "                        spans list(Span): разметка из датасета в формате Span(start, stop, type)\n",
    "                        offset_mapping list(tuple): карта сдвигов о соответствии символов в исходном тексте и токенов в токенизированном представлении текста\n",
    "                Возвращаемое значение:\n",
    "                        annotations list(string): разметка токенов в формате {'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O'}\n",
    "        \"\"\"\"\n",
    "        annotations = [\"O\"] * len(offset_mapping)\n",
    "        \n",
    "        for span in spans:\n",
    "            start, stop, entity_type = span.start, span.stop, span.type\n",
    "            start_token = None\n",
    "            end_token = None\n",
    "            for i, (token_start, token_end) in enumerate(offset_mapping):\n",
    "                if token_start is not None and start >= token_start and start < token_end:\n",
    "                    start_token = i\n",
    "                if token_start is not None and stop > token_start and stop <= token_end:\n",
    "                    end_token = i + 1\n",
    "                    \n",
    "            if start_token is not None and end_token is not None:\n",
    "                annotations[start_token] = \"B-\" + entity_type\n",
    "                for i in range(start_token + 1, end_token):\n",
    "                    annotations[i] = \"I-\" + entity_type\n",
    "    \n",
    "        return annotations\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data[index].text\n",
    "        spans = self.data[index].spans\n",
    "\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                return_offsets_mapping=True,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                max_length=self.max_len)\n",
    "        \n",
    "        annotations = self.annotate_entities(spans, encoding[\"offset_mapping\"])\n",
    "        labels = [labels_to_ids[label] for label in annotations]\n",
    "\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(labels)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c4124-f236-4713-a50a-fc9e5b05022a",
   "metadata": {},
   "source": [
    "Обернем датасет в модуль *LightningDataModule*. Опишем в нем датасеты и Dataloader'ы для всех этапов. *LightningDataModule* легко интегрируется с Trainer из PyTorch Lightning, который взаимодействует с DataModule для автоматического масштабирования данных и обучения на нескольких устройствах или узлах.  Не надо беспокоиться о передаче данных вручную, так как PyTorch Lightning обрабатывает большую часть логики обучения. Так же он может быть адаптирован для поддержки загрузки данных в распределенной среде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d4dc82-c167-45da-a851-b6c260c268a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERUSDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 train_data: list, #набор обучающих данных\n",
    "                 val_data: list,   #набор валидационных данных\n",
    "                 test_data: list,  #набор тестовых данных\n",
    "                 tokenizer: BertTokenizerFast, #токенайзер\n",
    "                 batch_size: int = 32,         #размер батча\n",
    "                 max_token_len: int = 512      #максимальная длин последовательности\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = NERUSdataset(\n",
    "            self.train_data,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        self.val_dataset = NERUSdataset(\n",
    "            self.val_data,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        self.test_dataset = NERUSdataset(\n",
    "            self.test_data,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2\n",
    "        )\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=2\n",
    "        )\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abc1b6-bc38-4aaf-b33f-96ae66571b65",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую, валидационную и тестовую выборки. Для обучения возьмем не 740К текстов, а 40К."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b53b36-fcf2-4878-aa00-8cdaec8d76a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28800, 7200, 4000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_data, test_data = train_test_split(data[:40000], test_size=0.1)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.2)\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e2d09-a2f7-4ec1-8277-1f2179dd896a",
   "metadata": {},
   "source": [
    "Определим необходимые параметры для обучения, а также DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cf2b971-79bf-49bd-90d9-c881b9fb9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 512\n",
    "BERT_MODEL_NAME = 'ai-forever/ruBert-base'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL_NAME)\n",
    "data_module = NERUSDataModule(\n",
    "    train_data,\n",
    "    val_data,\n",
    "    test_data,\n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_token_len=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97671a3d-67ad-4025-a7a5-83bdf016cc72",
   "metadata": {},
   "source": [
    "## Подготовка модели\n",
    "\n",
    "Напишем модуль *LightningModule* для реализации модели. Он определяет стандартный интерфейс для всех этапов обучения что облегчает реализацию обучения, валидации и тестирования. *LightningModule* интегрируется легко с PyTorch Lightning Trainer, и поэтому я легко использую распределенное обучение, логирование, сохранение чекпойнтов и т. д.  Внутри него инициализируем ***BertForTokenClassification*** с весами от *ruBERT*, указав новое количество классов в классификаторе. Реализуем все этапы обучения модели, подсчет метрик, прямой прогон через BERT и конфигурацию оптимизатора с шедулером. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44333582-c010-4fe4-93d4-f17e7f8e2fc5",
   "metadata": {},
   "source": [
    "Для оценки качества моделей в задаче Named Entity Recognition (NER) используют метрики:\r\n",
    "\r\n",
    "- **Entity-Level Micro/Macro F1-score**\r\n",
    "\r\n",
    "В качестве метрики будем использовать её. В наших данных будет наблюдатсья сильный дизбаланс классов, т.к. сущностей с меткой 'O' будет больше всего. Micro F1-score рассчитывается на уровне всего набора данных, объединяя предсказания и истинные метки для каждой сущности. Это подразумевает, что все предсказанные и истинные метки рассматриваются как один большой класс. Macro F1-score рассчитывается, усредняя F1-score для каждой индивидуальной сущности. Так мы можем равномерно оценить качество извлечения для всех сущностей, даже если некоторые из них представлены в данных редко.\r\n",
    "\r\n",
    "- **Exact Match**\r\n",
    "\r\n",
    "Измеряет процент точного совпадения между предсказанными и действительными именованными сущностями. Полезен, когда важно достичь точного соответствия.\r\n",
    "\r\n",
    "- **IOB Metrics** (IOB Precision, IOB Recall, IOB F1-score):\r\n",
    "\r\n",
    "Рассматривает только те предсказанные именованные сущности, которые правильно начинаются и заканчиваются.\r\n",
    "\r\n",
    "- **Span-Level Metrics**\r\n",
    "\r\n",
    "Span Precision, Span Recall, Span F1-score оценивают точность, полноту и F1-score, сосредотачиваясь на предсказанных именованных сущностях как на целых фрагментах текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5c14add-a2f6-4e15-a165-f2af74ee7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERTagger(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 n_training_steps: int, #количество шагов обучения\n",
    "                 n_warmup_steps: int,   #количество разминочных шагов \n",
    "                 learning_rate: float   #скорость обучения\n",
    "                ): \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() #сохраняем гиперпараметры для воспроизведения обучения\n",
    "        self.bert = BertForTokenClassification.from_pretrained(BERT_MODEL_NAME, num_labels=len(labels_to_ids))\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        #метрики для каждого этапа\n",
    "        self.train_f1_macro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='macro')\n",
    "        self.train_f1_micro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='micro')\n",
    "        self.val_f1_macro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='macro')\n",
    "        self.val_f1_micro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='micro')\n",
    "        self.test_f1_macro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='macro')\n",
    "        self.test_f1_micro = F1Score(task=\"multiclass\", num_classes=len(labels_to_ids), average='micro')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = output.logits\n",
    "        loss = output.loss\n",
    "        return logits, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]   #айди токенов\n",
    "        attention_mask = batch[\"attention_mask\"] #маска внимания\n",
    "        labels = batch[\"labels\"] #лейблы токенов\n",
    "        logits, loss = self(input_ids, attention_mask, labels)\n",
    "\n",
    "        #перевод таргетов и предсказаний в удобный формат для метрик\n",
    "        targets = labels.view(-1)\n",
    "        logits = logits.view(-1, len(unique_tags))\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "        \n",
    "        self.train_f1_macro(preds, targets)\n",
    "        self.train_f1_micro(preds, targets)\n",
    "        \n",
    "        self.log('train_f1_macro', self.train_f1_macro, logger=True, on_step=True, on_epoch=False)\n",
    "        self.log('train_f1_micro', self.train_f1_micro, logger=True, on_step=True, on_epoch=False)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True, sync_dist=True)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]   #айди токенов\n",
    "        attention_mask = batch[\"attention_mask\"] #маска внимания\n",
    "        labels = batch[\"labels\"] #лейблы токенов\n",
    "        logits, loss = self(input_ids, attention_mask, labels)\n",
    "\n",
    "        #перевод таргетов и предсказаний в удобный формат для метрик\n",
    "        targets = labels.view(-1)\n",
    "        logits = logits.view(-1, len(unique_tags))\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "\n",
    "        self.val_f1_macro(preds, targets)\n",
    "        self.val_f1_micro(preds, targets)\n",
    "        \n",
    "        self.log('val_f1_macro', self.val_f1_micro, logger=True, on_step=True, on_epoch=False)\n",
    "        self.log('val_f1_micro', self.val_f1_micro, logger=True, on_step=True, on_epoch=False)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True, sync_dist=True)\n",
    "        return loss\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]   #айди токенов\n",
    "        attention_mask = batch[\"attention_mask\"] #маска внимания\n",
    "        labels = batch[\"labels\"] #лейблы токенов\n",
    "        logits, loss = self(input_ids, attention_mask, labels)\n",
    "\n",
    "        #перевод таргетов и предсказаний в удобный формат для метрик\n",
    "        targets = labels.view(-1)\n",
    "        logits = logits.view(-1, len(unique_tags))\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "\n",
    "        self.test_f1_macro(preds, targets)\n",
    "        self.test_f1_micro(preds, targets)\n",
    "        \n",
    "        self.log('test_f1_macro', self.test_f1_macro)\n",
    "        self.log('test_f1_micro', self.test_f1_micro)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a6ef7-d858-4a7f-b373-2bc3e28235af",
   "metadata": {},
   "source": [
    "Подсчитаем количество шагов обучения и разминки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d914d2fb-3276-488f-8dbf-2bc9567c7f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 18000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "steps_per_epoch=len(train_data) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66db4be-a8a8-4aab-8fa4-5e8ba03cae6d",
   "metadata": {},
   "source": [
    "Пропишем сохранение чекпоинтов и логгирование в TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad256025-fcd4-4feb-87e2-07d327adcaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохранение чекпоинта каждый 1000 шаг обучения\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"../model/checkpoints\",\n",
    "    filename=\"NERTagger-{epoch}-{step}-{val_loss:.4f}\",\n",
    "    auto_insert_metric_name=True,\n",
    "    every_n_train_steps=1000,\n",
    "    verbose=True,\n",
    "    save_top_k=-1\n",
    ")\n",
    "\n",
    "#сохранение лучшего чекпоинта по валидационному лоссу\n",
    "best_checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"../model/checkpoints\",\n",
    "    filename=\"NERTagger-BEST\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b247cff-aecf-413f-8e4c-40ade24f167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"../model/lightning_logs\", name=\"NER-tagger\") #для логирования используем TensorBoard\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10) #колбэк для ранней остановки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4a5ed-43d6-462a-ad95-4a28a0db3cdd",
   "metadata": {},
   "source": [
    "## Обучение модели\n",
    "\n",
    "Запустим обучение и протестируем обученную модель на тестовой выборке и своем предложении. Для обучения используем pl.Trainer, он редоставляет высокоуровневый интерфейс для управления обучением, валидацией, сохранением модели, поддержку распределенного обучения и автоматически логирует метрики. Также круто, что автоматически обрабатываются шедулеры и оптимизаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c9e1787-de2a-41c1-a420-492ebdbab2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[early_stopping_callback, checkpoint_callback, best_checkpoint_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    accelerator=\"gpu\",\n",
    "    devices = [0, 1],\n",
    "    log_every_n_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e71cfd3a-088c-4cc7-a202-f9dfd21d8de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /home/jovyan/DS-Cloud-test/model/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name           | Type                       | Params\n",
      "--------------------------------------------------------------\n",
      "0 | bert           | BertForTokenClassification | 177 M \n",
      "1 | train_f1_macro | MulticlassF1Score          | 0     \n",
      "2 | train_f1_micro | MulticlassF1Score          | 0     \n",
      "3 | val_f1_macro   | MulticlassF1Score          | 0     \n",
      "4 | val_f1_micro   | MulticlassF1Score          | 0     \n",
      "5 | test_f1_macro  | MulticlassF1Score          | 0     \n",
      "6 | test_f1_micro  | MulticlassF1Score          | 0     \n",
      "--------------------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "710.888   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d045ba71f0f740c0869113a0c4d8f146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 450: 'val_loss' reached 0.02100 (best 0.02100), saving model to '/home/jovyan/DS-Cloud-test/model/checkpoints/NERTagger-BEST.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 900: 'val_loss' reached 0.01586 (best 0.01586), saving model to '/home/jovyan/DS-Cloud-test/model/checkpoints/NERTagger-BEST.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 1350: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1800: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 2250: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 2700: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 3150: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 3600: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 4050: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 4500: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 4950: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 5400: 'val_loss' was not in top 1\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "model = NERTagger(\n",
    "    learning_rate=3e-4,\n",
    "    n_warmup_steps=warmup_steps,\n",
    "    n_training_steps=total_training_steps\n",
    ")\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "374f7119-7292-4d30-a622-fc7b5e2ecd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:232: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d807bb045d48689a118a1e12e84ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_f1_macro         0.9674067497253418\n",
      "      test_f1_micro          0.994613766670227\n",
      "        test_loss          0.023007836192846298\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_f1_macro': 0.9674067497253418,\n",
       "  'test_f1_micro': 0.994613766670227,\n",
       "  'test_loss': 0.023007836192846298}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fefbdc-ec2b-4847-9cf0-8da2adc147bf",
   "metadata": {},
   "source": [
    "## Валидация модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0ac7d-0841-438d-873c-494b686a8619",
   "metadata": {},
   "source": [
    "Для валидации сразу загрузим лучшие веса модели из чекпоинта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d4188c4-0398-4782-8794-fdc9506f74d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trained_model = NERTagger.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "trained_model.eval()\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422cbf1-9b01-458b-ae20-84564d8edd25",
   "metadata": {},
   "source": [
    "Прогоним тестовую выборку, чтобы получить предсказания и посчитать метрики для каждого типа сущности в отдельности при помощи *seqeval*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "88291875-e6b4-4ac2-af2f-cd2a0c837c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2048000, 2048000)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.setup()\n",
    "test_dataloader = data_module.test_dataloader()\n",
    "predictions = []\n",
    "targets = []\n",
    "for item in test_dataloader:\n",
    "    outputs = trained_model(item[\"input_ids\"].to('cuda:0'), attention_mask=item[\"attention_mask\"].to('cuda:0'))\n",
    "    logits = outputs[0]\n",
    "    logits = logits.view(-1, len(unique_tags))\n",
    "    preds = torch.argmax(logits, axis=1)\n",
    "    predictions += preds.cpu().tolist()\n",
    "    targets += item['labels'].flatten().tolist()\n",
    "len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e9535be5-e464-40d7-a667-24c2cfd47435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.98      0.97      0.97     19616\n",
      "         ORG       0.91      0.91      0.91     18305\n",
      "         PER       0.98      0.96      0.97     16157\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     54078\n",
      "   macro avg       0.96      0.95      0.95     54078\n",
      "weighted avg       0.95      0.95      0.95     54078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report([[ids_to_labels[i] for i in predictions]], [[ids_to_labels[i] for i in targets]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abe9dd-83ae-46c6-b0c2-7408a922a6c9",
   "metadata": {},
   "source": [
    "Как видно по метрикам модель хорошо обучилась, и правильно определяет типы сущностей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76495fd-3aa8-4f72-898f-d917c2fdd233",
   "metadata": {},
   "source": [
    "Теперь подадим модели на вход случайное предложение и посмотрим результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "69f90e69-618d-4c36-a257-d8518cd022b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beauty_visual(preds, offset_mapping):\n",
    "    \"\"\"\n",
    "    Собирает список тегов для отрисовки предикта\n",
    "            Параметры:\n",
    "                    preds list: список предиктов в формате (токен, тэг)\n",
    "                    offset_mapping list(tuple): карта сдвигов о соответствии символов в исходном тексте \n",
    "                                                и токенов в токенизированном представлении текста\n",
    "            Возвращаемое значение:\n",
    "                    prediction list: список предиктов c тегами\n",
    "    \"\"\"\n",
    "    prediction = ''\n",
    "    for token_pred, mapping in zip(preds, offset_mapping.squeeze().tolist()):\n",
    "        if token_pred[0][0] =='[':\n",
    "            continue\n",
    "        pred = ''\n",
    "        if token_pred[1][0] == 'B':\n",
    "            prediction += ' '\n",
    "            pred = token_pred[1][2:]\n",
    "        elif token_pred[1][0] == 'I' and token_pred[0][:2] != '##':\n",
    "            pred = '-' * len(token_pred[0]) + '-'\n",
    "        elif token_pred[0][:2] == '##' or token_pred[1][0] == 'I':\n",
    "            pred = '-' * len(token_pred[0][3:])\n",
    "        else:\n",
    "            prediction += ' '\n",
    "            pred = token_pred[1]\n",
    "        pred += (mapping[1]-mapping[0] - len(pred)) * '-'\n",
    "        prediction += pred\n",
    "    return prediction[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "19c60afa-89ef-4cfe-9c37-2dc7c9f04567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_step(sentence, trained_model: NERTagger):\n",
    "    \"\"\"\n",
    "    Выполняет инференс модели, прогоняя запрос через токенайзер и возвращая список токенов с их тэгами\n",
    "            Параметры:\n",
    "                    sentence string: предложение (запрос) для классификации сущностей\n",
    "                    trained_model NERTagger: обученная модель\n",
    "            Возвращаемое значение:\n",
    "                    preds list: список предиктов в формате (токен, тэг)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(sentence,\n",
    "                        return_offsets_mapping=True,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        max_length=512,\n",
    "                        return_tensors=\"pt\")\n",
    "    \n",
    "    ids = inputs[\"input_ids\"].to('cuda:0')\n",
    "    mask = inputs[\"attention_mask\"].to('cuda:0')\n",
    "    \n",
    "    outputs = trained_model(ids, attention_mask=mask)\n",
    "    logits = outputs[0]\n",
    "    logits = logits.view(-1, len(unique_tags))\n",
    "    preds = torch.argmax(logits, axis=1)\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids.view(-1))\n",
    "    token_predictions = [ids_to_labels[i] for i in preds.cpu().numpy()]\n",
    "    preds = list(zip(tokens, token_predictions)) \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "55ec3f1c-6ace-486f-b07f-d61ba17af1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Глава компании Apple Тим Кук осенью представил новый телефон в штаб квартире Apple Inc в Купертино\n",
      "O---- O------- ORG-- PER---- O----- O--------- O---- O------ O O--- O------- ORG------ O LOC------\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Глава компании Apple Тим Кук осенью представил новый телефон в штаб квартире Apple Inc в Купертино'\n",
    "\n",
    "preds = inference_step(sentence, trained_model)\n",
    "print(sentence)\n",
    "print(beauty_visual(preds, inputs[\"offset_mapping\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b79d8-d25a-4e2f-8b76-83d89f3baf47",
   "metadata": {},
   "source": [
    "Экспортируем модель в формат *BertForTokenClassification*, более удобный для инференса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c84445e4-a6f4-4619-aca3-cf3206285237",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.bert.save_pretrained('../model/NERtagger')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
